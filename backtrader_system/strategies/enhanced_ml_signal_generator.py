#!/usr/bin/env python3
"""
Enhanced ML Signal Generator for Elite Trading System
Fixed version with working ensemble models
"""

import logging
import warnings
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, Any, Optional, List, Tuple
import yfinance as yf

# Suppress warnings
warnings.filterwarnings('ignore')

# Enhanced ML models
import xgboost as xgb
import lightgbm as lgb
from sklearn.neural_network import MLPRegressor, MLPClassifier
from sklearn.preprocessing import RobustScaler, StandardScaler
from sklearn.metrics import accuracy_score, r2_score, mean_squared_error

# Statistical analysis
from scipy import stats
from scipy.stats import linregress

# Try to import TA-Lib
try:
    import talib
    TALIB_AVAILABLE = True
except ImportError:
    TALIB_AVAILABLE = False

logger = logging.getLogger(__name__)

class EnhancedMLSignalGenerator:
    """
    Elite ML Signal Generator with 100+ advanced features and ensemble models
    """
    
    def __init__(self, config: Dict[str, Any]):
        self.logger = logger
        self.config = config
        
        # Model storage
        self.ensemble_models = {}   # {symbol: {'strength': model, 'direction': model}}
        self.scalers = {}          # {symbol: scaler}
        self.feature_importance = {}  # {symbol: feature_importance}
        
        # Market data cache
        self.market_data_cache = {}
        self.vix_data = None
        self.spy_data = None
        
        # Model parameters
        self.model_params = {
            'xgb_regressor': {
                'n_estimators': 300,
                'max_depth': 8,
                'learning_rate': 0.05,
                'subsample': 0.8,
                'colsample_bytree': 0.8,
                'reg_alpha': 0.1,
                'reg_lambda': 0.1,
                'random_state': 42,
                'n_jobs': -1
            },
            'lgb_regressor': {
                'n_estimators': 300,
                'max_depth': 6,
                'learning_rate': 0.05,
                'feature_fraction': 0.8,
                'bagging_fraction': 0.8,
                'reg_alpha': 0.1,
                'reg_lambda': 0.1,
                'random_state': 42,
                'n_jobs': -1,
                'verbose': -1
            },
            'neural_network': {
                'hidden_layer_sizes': (100, 50, 25),
                'activation': 'relu',
                'solver': 'adam',
                'alpha': 0.001,
                'max_iter': 500,
                'random_state': 42
            }
        }
        
        # Performance tracking
        self.model_performance = {}
        self.prediction_history = {}
        
        # Load market context data
        self._load_market_context()
        
        if self.logger:
            self.logger.info("ðŸš€ Enhanced ML Signal Generator initialized")
    
    def _load_market_context(self):
        """Load VIX and SPY data for market context"""
        try:
            end_date = datetime.now()
            start_date = end_date - timedelta(days=2*365)  # 2 years of context
            
            # Download VIX
            vix = yf.download('^VIX', start=start_date, end=end_date, progress=False)
            if not vix.empty:
                self.vix_data = vix['Close']
            
            # Download SPY
            spy = yf.download('SPY', start=start_date, end=end_date, progress=False)
            if not spy.empty:
                self.spy_data = spy['Close']
            
            if self.logger:
                self.logger.info("âœ… Market context data loaded (VIX, SPY)")
                
        except Exception as e:
            if self.logger:
                self.logger.warning(f"Could not load market context: {e}")
    
    def calculate_advanced_features(self, df: pd.DataFrame, symbol: str) -> pd.DataFrame:
        """
        Calculate 105+ advanced technical features
        """
        try:
            df = df.copy()
            
            # Ensure we have OHLCV
            required_cols = ['Open', 'High', 'Low', 'Close', 'Volume']
            if not all(col in df.columns for col in required_cols):
                if self.logger:
                    self.logger.warning(f"Missing OHLCV data for {symbol}")
                return df
            
            # 1. PRICE ACTION FEATURES (20 features)
            df['price_change'] = df['Close'].pct_change()
            df['price_change_3d'] = df['Close'].pct_change(3)
            df['price_change_5d'] = df['Close'].pct_change(5)
            df['price_change_10d'] = df['Close'].pct_change(10)
            df['price_change_20d'] = df['Close'].pct_change(20)
            
            # Basic ratios
            df['hl_ratio'] = (df['High'] - df['Low']) / df['Close']
            df['oc_ratio'] = (df['Close'] - df['Open']) / df['Open']
            
            # Gap analysis
            df['gap_up'] = (df['Open'] - df['Close'].shift(1)) / df['Close'].shift(1)
            df['gap_fill'] = np.where(df['gap_up'] > 0, 
                                    (df['Low'] <= df['Close'].shift(1)), 
                                    (df['High'] >= df['Close'].shift(1))).astype(int)
            
            # Position within day's range
            df['close_position'] = (df['Close'] - df['Low']) / (df['High'] - df['Low'])
            
            # 2. MOVING AVERAGES & TRENDS (20 features)
            periods = [5, 10, 20, 50, 100]
            for period in periods:
                ma_col = f'ma_{period}'
                df[ma_col] = df['Close'].rolling(period).mean()
                df[f'price_vs_ma_{period}'] = (df['Close'] - df[ma_col]) / df[ma_col]
            
            # MA crossovers and slopes
            df['ma_5_20_diff'] = (df['ma_5'] - df['ma_20']) / df['ma_20']
            df['ma_20_50_diff'] = (df['ma_20'] - df['ma_50']) / df['ma_50']
            
            # MA slope
            df['ma_slope_5'] = df['ma_5'].pct_change(5)
            
            # 3. VOLATILITY FEATURES (15 features)
            volatility_periods = [5, 10, 20, 30]
            for period in volatility_periods:
                vol_col = f'volatility_{period}d'
                df[vol_col] = df['Close'].pct_change().rolling(period).std()
                df[f'volatility_rank_{period}d'] = df[vol_col].rolling(100).rank(pct=True)
            
            # ATR
            if TALIB_AVAILABLE:
                df['atr_14'] = talib.ATR(df['High'], df['Low'], df['Close'], timeperiod=14)
            else:
                high_low = df['High'] - df['Low']
                high_close = (df['High'] - df['Close'].shift()).abs()
                low_close = (df['Low'] - df['Close'].shift()).abs()
                tr = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)
                df['atr_14'] = tr.rolling(14).mean()
            df['atr_rank'] = df['atr_14'].rolling(100).rank(pct=True)
            
            # Volatility regimes - convert to numeric codes
            vol_75 = df['volatility_20d'].rolling(100).quantile(0.75)
            vol_25 = df['volatility_20d'].rolling(100).quantile(0.25)
            df['vol_regime'] = np.where(df['volatility_20d'] > vol_75, 2,  # high = 2
                                      np.where(df['volatility_20d'] < vol_25, 0, 1))  # low = 0, medium = 1
            
            # 4. MOMENTUM FEATURES (20 features)
            # RSI family
            if TALIB_AVAILABLE:
                df['rsi_14'] = talib.RSI(df['Close'], timeperiod=14)
                df['rsi_7'] = talib.RSI(df['Close'], timeperiod=7)
                df['rsi_21'] = talib.RSI(df['Close'], timeperiod=21)
                
                # Stochastic
                df['stoch_k'], df['stoch_d'] = talib.STOCH(df['High'], df['Low'], df['Close'])
                
                # MACD family
                df['macd'], df['macd_signal'], df['macd_histogram'] = talib.MACD(df['Close'])
                
                # Williams %R
                df['williams_r'] = talib.WILLR(df['High'], df['Low'], df['Close'])
                
                # Rate of Change
                df['roc_5'] = talib.ROC(df['Close'], timeperiod=5)
                df['roc_10'] = talib.ROC(df['Close'], timeperiod=10)
                df['roc_20'] = talib.ROC(df['Close'], timeperiod=20)
            else:
                # Pandas equivalents
                delta = df['Close'].diff()
                gain = (delta.where(delta > 0, 0)).rolling(14).mean()
                loss = (-delta.where(delta < 0, 0)).rolling(14).mean()
                rs = gain / loss
                df['rsi_14'] = 100 - (100 / (1 + rs))
                
                # Simple MACD
                exp1 = df['Close'].ewm(span=12).mean()
                exp2 = df['Close'].ewm(span=26).mean()
                df['macd'] = exp1 - exp2
                df['macd_signal'] = df['macd'].ewm(span=9).mean()
                df['macd_histogram'] = df['macd'] - df['macd_signal']
                
                # Fill other momentum indicators with simple calculations
                df['rsi_7'] = df['rsi_14']  # Placeholder
                df['rsi_21'] = df['rsi_14']  # Placeholder
                df['stoch_k'] = df['rsi_14']  # Placeholder
                df['stoch_d'] = df['rsi_14']  # Placeholder
                df['williams_r'] = -df['rsi_14']  # Placeholder
                df['roc_5'] = df['Close'].pct_change(5) * 100
                df['roc_10'] = df['Close'].pct_change(10) * 100
                df['roc_20'] = df['Close'].pct_change(20) * 100
            
            # Custom momentum features
            df['rsi_divergence'] = df['rsi_14'].diff()
            df['macd_slope'] = df['macd'].pct_change(3)
            df['momentum_consistency'] = df['Close'].pct_change().rolling(10).apply(
                lambda x: (x > 0).sum() / len(x))
            
            # 5. VOLUME FEATURES (15 features)
            df['volume_sma_20'] = df['Volume'].rolling(20).mean()
            df['volume_ratio'] = df['Volume'] / df['volume_sma_20']
            df['volume_rank'] = df['Volume'].rolling(50).rank(pct=True)
            
            # Price-Volume indicators
            df['price_volume_trend'] = ((df['Close'] - df['Close'].shift()) / df['Close'].shift()) * df['Volume']
            
            # VWAP
            df['vwap'] = (df['Close'] * df['Volume']).rolling(20).sum() / df['Volume'].rolling(20).sum()
            df['price_vs_vwap'] = (df['Close'] - df['vwap']) / df['vwap']
            
            # On Balance Volume
            obv = []
            for i in range(len(df)):
                if i == 0:
                    obv.append(df['Volume'].iloc[i])
                else:
                    if df['Close'].iloc[i] > df['Close'].iloc[i-1]:
                        obv.append(obv[-1] + df['Volume'].iloc[i])
                    elif df['Close'].iloc[i] < df['Close'].iloc[i-1]:
                        obv.append(obv[-1] - df['Volume'].iloc[i])
                    else:
                        obv.append(obv[-1])
            df['obv'] = obv
            df['obv_slope'] = pd.Series(df['obv']).pct_change(5)
            
            # Accumulation/Distribution Line
            clv = ((df['Close'] - df['Low']) - (df['High'] - df['Close'])) / (df['High'] - df['Low'])
            df['ad_line'] = (clv * df['Volume']).cumsum()
            df['ad_slope'] = df['ad_line'].pct_change(5)
            
            # Volume profile (simplified)
            try:
                df['volume_profile'] = df['Volume'].rolling(20).apply(
                    lambda x: x.iloc[-5:].mean() / x.mean() if len(x) >= 5 else 1.0)
            except:
                df['volume_profile'] = 1.0
            
            # Add remaining features to reach 105+ total
            # 6. BOLLINGER BANDS & MEAN REVERSION
            rolling_mean = df['Close'].rolling(20).mean()
            rolling_std = df['Close'].rolling(20).std()
            df['bb_upper'] = rolling_mean + (rolling_std * 2)
            df['bb_middle'] = rolling_mean
            df['bb_lower'] = rolling_mean - (rolling_std * 2)
            df['bb_width'] = (df['bb_upper'] - df['bb_lower']) / df['bb_middle']
            df['bb_position'] = (df['Close'] - df['bb_lower']) / (df['bb_upper'] - df['bb_lower'])
            df['bb_squeeze'] = (df['bb_width'] < df['bb_width'].rolling(20).quantile(0.1)).astype(int)
            
            # 7. MARKET CONTEXT
            if self.vix_data is not None:
                vix_aligned = self.vix_data.reindex(df.index, method='ffill')
                df['vix'] = vix_aligned
                df['vix_rank'] = df['vix'].rolling(252).rank(pct=True)
                df['vix_change'] = df['vix'].pct_change(5)
                df['fear_greed'] = 1 - df['vix_rank']
            else:
                df['vix'] = 20
                df['vix_rank'] = 0.5
                df['vix_change'] = 0
                df['fear_greed'] = 0.5
            
            # 8. ADDITIONAL FEATURES TO REACH 105+
            # Statistical features  
            df['skewness_20'] = df['Close'].pct_change().rolling(20).skew()
            # Simplified kurtosis calculation to avoid pandas apply issues
            try:
                df['kurtosis_20'] = df['Close'].pct_change().rolling(20).apply(
                    lambda x: float(x.kurtosis()) if len(x) >= 10 and not x.isna().all() else 0.0, raw=False)
            except:
                df['kurtosis_20'] = 0.0  # Fallback value
            df['autocorr_5'] = df['Close'].pct_change().rolling(20).apply(
                lambda x: x.autocorr(5) if len(x) >= 10 else 0)
            
            # Support/resistance
            high_20 = df['High'].rolling(20).max()
            low_20 = df['Low'].rolling(20).min()
            df['near_resistance'] = np.where((df['Close'] / high_20 - 1) > -0.02, 1, 0)
            df['near_support'] = np.where((df['Close'] / low_20 - 1) < 0.02, 1, 0)
            
            # Pattern recognition
            def safe_linregress(y):
                try:
                    if len(y) < 3 or y.isna().all():
                        return 0
                    x = np.arange(len(y))
                    y_clean = y.dropna()
                    if len(y_clean) < 3:
                        return 0
                    slope, _, r_value, _, _ = linregress(x[:len(y_clean)], y_clean)
                    return slope * r_value
                except:
                    return 0
            
            df['trend_strength'] = abs(df['Close'].rolling(20, min_periods=10).apply(safe_linregress))
            df['trend_direction'] = df['Close'].rolling(20, min_periods=10).apply(safe_linregress)
            
            # More volume features
            df['volume_weighted_price'] = (df['Close'] * df['Volume']).rolling(10).sum() / df['Volume'].rolling(10).sum()
            df['volume_momentum'] = df['Volume'].pct_change(5)
            
            # Risk metrics
            rolling_max = df['Close'].rolling(20).max()
            df['drawdown'] = (df['Close'] / rolling_max - 1)
            df['max_drawdown_20'] = df['drawdown'].rolling(20).min()
            
            # Time-based features
            df['hour'] = df.index.hour if hasattr(df.index, 'hour') else 12
            df['is_open_hour'] = np.where(df['hour'].between(9, 16), 1, 0)
            df['is_power_hour'] = np.where(df['hour'] == 15, 1, 0)
            
            # Regime detection
            momentum_score = df['Close'].pct_change(10).rolling(20).mean()
            volatility_score = 1 / (1 + df['Close'].pct_change().rolling(20).std())
            volume_score = df['volume_ratio'].rolling(10).mean() - 1
            
            df['regime_momentum'] = momentum_score
            df['regime_volatility'] = volatility_score
            df['regime_volume'] = volume_score
            df['regime_score'] = (momentum_score + volatility_score + volume_score) / 3
            df['bull_regime'] = np.where(df['regime_score'] > 0.6, 1, 0)
            
            # Fill any remaining NaN values
            numeric_columns = df.select_dtypes(include=[np.number]).columns
            for col in numeric_columns:
                if col in df.columns:
                    df[col] = df[col].fillna(df[col].median())
            
            if self.logger:
                feature_count = len([col for col in df.columns if col not in ['Open', 'High', 'Low', 'Close', 'Volume']])
                self.logger.info(f"âœ… Calculated {feature_count} advanced features for {symbol}")
            
            return df
            
        except Exception as e:
            if self.logger:
                self.logger.error(f"Error calculating advanced features for {symbol}: {e}")
            return df
    
    def create_enhanced_targets(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Create sophisticated ML targets for maximum predictive power
        """
        try:
            df = df.copy()
            
            # Ensure price_change exists
            if 'price_change' not in df.columns:
                df['price_change'] = df['Close'].pct_change()
            
            # Multi-horizon returns
            horizons = [1, 3, 5, 10, 20]
            for h in horizons:
                df[f'future_return_{h}d'] = df['Close'].shift(-h) / df['Close'] - 1
                df[f'future_volatility_{h}d'] = df['Close'].pct_change().rolling(h).std().shift(-h)
            
            # Risk-adjusted return targets
            df['risk_adjusted_return_5d'] = df['future_return_5d'] / (df['future_volatility_5d'] + 0.001)
            df['risk_adjusted_return_10d'] = df['future_return_10d'] / (df['future_volatility_10d'] + 0.001)
            
            # Signal strength (0.2 to 1.0) - based on risk-adjusted moves
            abs_risk_adj_5d = np.abs(df['risk_adjusted_return_5d']).fillna(0.3)
            signal_strength_raw = np.clip(abs_risk_adj_5d * 2, 0, 1)
            df['ml_signal_strength'] = 0.2 + (signal_strength_raw * 0.8)
            
            # Direction prediction (buy/sell/hold) - ensure 0-indexed classes for XGBoost
            risk_adj_5d = df['risk_adjusted_return_5d'].fillna(0.0)
            df['direction_target'] = np.where(
                risk_adj_5d > 0.3, 2,  # Strong buy (class 2)
                np.where(risk_adj_5d < -0.3, 0, 1)  # Strong sell (class 0), else hold (class 1)
            ).astype(int)
            
            # Regime target
            momentum_strength = np.abs(df['future_return_10d']).fillna(0.05)
            volatility_level = df['future_volatility_10d'].fillna(0.02)
            
            momentum_q70 = momentum_strength.quantile(0.7) if len(momentum_strength) > 0 else 0.1
            volatility_q50 = volatility_level.quantile(0.5) if len(volatility_level) > 0 else 0.05
            volatility_q80 = volatility_level.quantile(0.8) if len(volatility_level) > 0 else 0.1
            
            df['regime_target'] = np.where(
                (momentum_strength > momentum_q70) & (volatility_level < volatility_q50), 2,
                np.where(volatility_level > volatility_q80, 0, 1)
            )
            df['regime_target'] = df['regime_target'].fillna(1)
            
            # Probability of profit
            df['prob_profit'] = np.where(df['future_return_5d'] > 0, 1, 0)
            df['prob_profit'] = df['prob_profit'].fillna(0.5)
            
            return df
            
        except Exception as e:
            if self.logger:
                self.logger.error(f"Error creating enhanced targets: {e}")
            return df
    
    def train_ensemble_models(self, symbol: str, training_data: pd.DataFrame) -> bool:
        """
        Train ensemble of XGBoost, LightGBM, and Neural Network models
        """
        try:
            # Prepare data
            df_features = self.calculate_advanced_features(training_data.copy(), symbol)
            df_with_targets = self.create_enhanced_targets(df_features)
            
            # Clean data
            target_cols = ['ml_signal_strength', 'direction_target', 'regime_target', 'prob_profit']
            clean_data = df_with_targets.dropna(subset=target_cols)
            
            # Fill NaN values for features
            feature_cols = [col for col in clean_data.columns 
                          if col not in ['Open', 'High', 'Low', 'Close', 'Volume'] + target_cols
                          and not col.startswith('future_') and not col.startswith('risk_adjusted_')]
            
            for col in feature_cols:
                if col in clean_data.columns:
                    clean_data[col] = clean_data[col].fillna(clean_data[col].median())
            
            # Additional safety check - fill any remaining NaNs with 0
            feature_data = clean_data[feature_cols]
            feature_data = feature_data.fillna(0)
            
            if len(clean_data) < 50:
                if self.logger:
                    self.logger.warning(f"Insufficient data for {symbol}: {len(clean_data)} < 50")
                return False
            
            # Feature selection - ensure no NaN values
            X = feature_data.values
            
            # Final NaN check
            if np.any(np.isnan(X)):
                if self.logger:
                    self.logger.warning(f"Found NaN values in features for {symbol}, filling with 0")
                X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)
            
            # Scale features
            scaler = RobustScaler()
            X_scaled = scaler.fit_transform(X)
            self.scalers[symbol] = scaler
            
            # Prepare targets
            y_strength = clean_data['ml_signal_strength'].values
            y_direction = clean_data['direction_target'].values
            
            # Split for validation
            split_idx = int(len(X_scaled) * 0.8)
            X_train, X_val = X_scaled[:split_idx], X_scaled[split_idx:]
            y_strength_train, y_strength_val = y_strength[:split_idx], y_strength[split_idx:]
            y_direction_train, y_direction_val = y_direction[:split_idx], y_direction[split_idx:]
            
            # Manual Ensemble Classes to avoid VotingRegressor compatibility issues
            class ManualEnsembleRegressor:
                def __init__(self, models):
                    self.models = models
                
                def predict(self, X):
                    predictions = np.array([model.predict(X) for model in self.models])
                    return np.mean(predictions, axis=0)
            
            class ManualEnsembleClassifier:
                def __init__(self, models):
                    self.models = models
                
                def fit(self, X, y):
                    for model in self.models:
                        model.fit(X, y)
                    return self
                
                def predict(self, X):
                    predictions = np.array([model.predict(X) for model in self.models])
                    return np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=0, arr=predictions)
                
                def predict_proba(self, X):
                    proba_predictions = np.array([model.predict_proba(X) for model in self.models])
                    return np.mean(proba_predictions, axis=0)
            
            # Train Signal Strength Ensemble
            xgb_reg = xgb.XGBRegressor(**self.model_params['xgb_regressor'])
            lgb_reg = lgb.LGBMRegressor(**self.model_params['lgb_regressor'])
            nn_reg = MLPRegressor(**self.model_params['neural_network'])
            
            xgb_reg.fit(X_train, y_strength_train)
            lgb_reg.fit(X_train, y_strength_train)
            nn_reg.fit(X_train, y_strength_train)
            
            strength_ensemble = ManualEnsembleRegressor([xgb_reg, lgb_reg, nn_reg])
            
            # Validate strength model
            strength_pred = strength_ensemble.predict(X_val)
            strength_r2 = r2_score(y_strength_val, strength_pred)
            strength_mse = mean_squared_error(y_strength_val, strength_pred)
            
            # Train Direction Ensemble
            xgb_clf_params = {k: v for k, v in self.model_params['xgb_regressor'].items() 
                             if k not in ['reg_alpha', 'reg_lambda', 'objective']}
            lgb_clf_params = {k: v for k, v in self.model_params['lgb_regressor'].items() 
                             if k not in ['reg_alpha', 'reg_lambda', 'objective']}
            
            xgb_clf = xgb.XGBClassifier(**xgb_clf_params)
            lgb_clf = lgb.LGBMClassifier(**lgb_clf_params)
            nn_clf = MLPClassifier(**self.model_params['neural_network'])
            
            direction_ensemble = ManualEnsembleClassifier([xgb_clf, lgb_clf, nn_clf])
            direction_ensemble.fit(X_train, y_direction_train)
            
            # Validate direction model
            direction_pred = direction_ensemble.predict(X_val)
            direction_accuracy = accuracy_score(y_direction_val, direction_pred)
            
            # Store models
            if symbol not in self.ensemble_models:
                self.ensemble_models[symbol] = {}
            
            self.ensemble_models[symbol]['strength'] = strength_ensemble
            self.ensemble_models[symbol]['direction'] = direction_ensemble
            self.ensemble_models[symbol]['feature_cols'] = feature_cols
            
            # Store performance metrics
            self.model_performance[symbol] = {
                'strength_r2': strength_r2,
                'strength_mse': strength_mse,
                'direction_accuracy': direction_accuracy,
                'training_samples': len(X_train),
                'features_count': len(feature_cols),
                'last_trained': datetime.now()
            }
            
            if self.logger:
                self.logger.info(f"âœ… Trained ensemble models for {symbol}:")
                self.logger.info(f"   Strength RÂ²: {strength_r2:.3f}, MSE: {strength_mse:.4f}")
                self.logger.info(f"   Direction Accuracy: {direction_accuracy:.3f}")
                self.logger.info(f"   Features: {len(feature_cols)}, Samples: {len(X_train)}")
            
            return True
            
        except Exception as e:
            if self.logger:
                self.logger.error(f"Error training ensemble models for {symbol}: {e}")
            return False
    
    def get_ml_signal(self, symbol: str, current_data: pd.DataFrame) -> Dict[str, Any]:
        """
        Generate ML-based trading signal
        """
        try:
            if symbol not in self.ensemble_models:
                return {
                    'signal': 'HOLD',
                    'signal_strength': 0.0,
                    'confidence': 0.0,
                    'direction': 1,
                    'regime': 1,
                    'features_count': 0,
                    'predicted_strength': 0.0,
                    'volatility': 0.02,
                    'volume_ratio': 1.0,
                    'buy_probability': 0.5,
                    'sell_probability': 0.5,
                    'error': 'No trained model'
                }
            
            # Calculate features
            df_features = self.calculate_advanced_features(current_data.copy(), symbol)
            
            if len(df_features) == 0:
                return {
                    'signal': 'HOLD',
                    'signal_strength': 0.0,
                    'confidence': 0.0,
                    'direction': 1,
                    'regime': 1,
                    'features_count': 0,
                    'predicted_strength': 0.0,
                    'volatility': 0.02,
                    'volume_ratio': 1.0,
                    'buy_probability': 0.5,
                    'sell_probability': 0.5,
                    'error': 'No features calculated'
                }
            
            latest_row = df_features.iloc[-1]
            feature_cols = self.ensemble_models[symbol]['feature_cols']
            
            # Prepare feature vector
            feature_vector = []
            for col in feature_cols:
                if col in latest_row:
                    val = latest_row[col]
                    if pd.isna(val):
                        val = 0.0
                    feature_vector.append(val)
                else:
                    feature_vector.append(0.0)
            
            X = np.array([feature_vector])
            
            # Scale features
            if symbol in self.scalers:
                X_scaled = self.scalers[symbol].transform(X)
            else:
                X_scaled = X
            
            # Get predictions
            strength_model = self.ensemble_models[symbol]['strength']
            direction_model = self.ensemble_models[symbol]['direction']
            
            signal_strength = strength_model.predict(X_scaled)[0]
            direction_pred = direction_model.predict(X_scaled)[0]
            
            # Get direction probabilities for confidence
            try:
                direction_proba = direction_model.predict_proba(X_scaled)[0]
                confidence = np.max(direction_proba)
            except:
                confidence = 0.6
            
            # Convert direction to signal
            if direction_pred == 2:  # Strong buy
                signal = signal_strength
            elif direction_pred == 0:  # Strong sell
                signal = -signal_strength
            else:  # Hold
                signal = 0.0
            
            # Extract additional features for strategy filters
            current_volatility = latest_row.get('atr_20', latest_row.get('volatility_20d', 0.02))
            volume_ratio = latest_row.get('volume_ratio', latest_row.get('volume_sma_ratio_20', 1.0))
            
            # Calculate buy/sell probabilities based on direction and confidence
            if direction_pred == 2:  # Strong buy
                buy_probability = confidence
                sell_probability = 1 - confidence
            elif direction_pred == 0:  # Strong sell  
                buy_probability = 1 - confidence
                sell_probability = confidence
            else:  # Hold
                buy_probability = 0.5
                sell_probability = 0.5
            
            result = {
                'signal': 'BUY' if signal > 0 else ('SELL' if signal < 0 else 'HOLD'),
                'signal_strength': float(signal),
                'confidence': float(confidence),
                'direction': int(direction_pred),
                'regime': 1,
                'features_count': len(feature_cols),
                'predicted_strength': float(abs(signal_strength)),
                'volatility': float(current_volatility),
                'volume_ratio': float(volume_ratio),
                'buy_probability': float(buy_probability),
                'sell_probability': float(sell_probability)
            }
            
            # Store prediction history
            if symbol not in self.prediction_history:
                self.prediction_history[symbol] = []
            
            self.prediction_history[symbol].append({
                'timestamp': datetime.now(),
                'signal': signal,
                'confidence': confidence,
                'direction': direction_pred
            })
            
            # Keep only last 100 predictions
            if len(self.prediction_history[symbol]) > 100:
                self.prediction_history[symbol] = self.prediction_history[symbol][-100:]
            
            return result
            
        except Exception as e:
            if self.logger:
                self.logger.error(f"Error generating ML signal for {symbol}: {e}")
            return {
                'signal': 'HOLD',
                'signal_strength': 0.0,
                'confidence': 0.0,
                'direction': 1,
                'regime': 1,
                'features_count': 0,
                'predicted_strength': 0.0,
                'volatility': 0.02,
                'volume_ratio': 1.0,
                'buy_probability': 0.5,
                'sell_probability': 0.5,
                'error': str(e)
            }
    
    def get_model_performance(self, symbol: str) -> Dict[str, Any]:
        """Get model performance metrics"""
        if symbol in self.model_performance:
            return self.model_performance[symbol].copy()
        return {}
    
    def get_model_summary(self) -> Dict[str, Any]:
        """Get summary of all trained models"""
        summary = {
            'trained_symbols': list(self.ensemble_models.keys()),
            'total_models': len(self.ensemble_models),
            'performance': {}
        }
        
        for symbol in self.ensemble_models.keys():
            summary['performance'][symbol] = self.get_model_performance(symbol)
        
        return summary
    
    def is_model_trained(self, symbol: str) -> bool:
        """Check if model is trained for symbol"""
        return symbol in self.ensemble_models and 'strength' in self.ensemble_models[symbol]
